---
published: false
---
## Binary Classification with MC-Dropout Models

An introduction to classification with Bayesian Deep Learning with the Monte-Carlo Dropout implementation.  

### Introduction

Uncertainty measurement for Deep Learning models may play an important role in the context of risk management. Still, to understand the mechanism and interpretation of the aleatoric and epistemic uncertainty, we first analyze them in a simpler and risk-less context, the **cats vs. dogs image classification.**

![](https://api.wandb.ai/files/sborquez/images/projects/131709/a91e89f8.png)

The submodels [implementation](https://github.com/sborquez/her2bdl/tree/dev) allows us to use any pre-trained model as the encoder model $E$, for this dataset, it is convenient to use a well-known model for image classification. In particular, we use the Keras implementation of the **EfficientNet**, with the configuration **B0** and the weights trained with the **ImageNet dataset** (https://image-net.org/).
For the stochastic classifier model $C$, we implement it by stacking three fully dense connection layers. As the MC-Dropout model requires, a dropout layer is added between each dense layer to enforce the stochastic output.  The following figure shows the architecture of the model used for this binary classification problem.

![nn-EfficientNet.png]({{site.baseurl}}/_drafts/nn-EfficientNet.png)

In the rest of this post, we will discuss the training procedure, the classification performance and analyze and interpretation of the uncertainty.




