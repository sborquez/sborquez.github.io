---
published: false
---
## Binary Classification with MC-Dropout Models

An introduction to classification with Bayesian Deep Learning with the Monte-Carlo Dropout implementation. 

**Check the [interactive version](https://wandb.ai/sborquez/her2bdl/reports/Binary-Classification-with-MC-Dropout-Models--VmlldzozMTg0OTE) on the Weight & Biases plataform**.

### Introduction

Bayesian Deep Learning (BDL)[1] allows to include the uncertainty measurement for Deep Learning (DL) models. Quantify the confidence of the model may play an important role in the context of risk management. Different BDL methods have been developed in recent years to estimate uncertainty [2,3]. Still, to understand the mechanism and interpretation of the aleatoric and epistemic uncertainty [4], we first analyze them in a simpler and risk-less context, the **cats vs. dogs image classification.**


![](https://api.wandb.ai/files/sborquez/images/projects/131709/a91e89f8.png)

The proposed submodels [implementation](https://github.com/sborquez/her2bdl/tree/dev) of the Monte-Carlo dropout [2], allows us to use any pre-trained model as the encoder model _E_, for this dataset, it is convenient to use a well-known model for image classification. In particular, we use the Keras implementation of the **EfficientNet**, with the configuration **B0** and the weights trained with the **ImageNet dataset** (https://image-net.org/).
For the stochastic classifier model _C_, we implement it by stacking three fully dense connection layers. As the MC-Dropout model requires, a dropout layer is added between each dense layer to enforce the stochastic output.  The following figure shows the architecture of the model used for this binary classification problem.

![nn-EfficientNet.png]({{site.baseurl}}/_drafts/nn-EfficientNet.png)

In the rest of this post, we will discuss the training procedure, the classification performance and analyze and interpretation of the uncertainty.

### Training Procedure

The most important feature of an MC-Dropout model is the seamless integration with the most commonly used deep learning frameworks, especially the no alteration of the training procedure. It can fit its weight by the classics backpropagation algorithms. The only modification in the architecture is the addition of the dropout layer, which is trained as usual, i.e., for each step, we take a sample of the weights by dropping some connections on the network and propagating the error through the remaining connections.
The model is trained with the RMSProp algorithm, with a learning rate of 0.001, preliminary experiments have shown that the SGD and moment base backpropagate algorithms results in underfitting and models with a low generalization.

### Classification Metrics

In general, the performance metrics show that this model can perfectly classify images of cats and dogs.

![binary_training.png]({{site.baseurl}}/_drafts/binary_training.png)
![clasify.png]({{site.baseurl}}/_drafts/clasify.png)

### Prediction Examples

An MC-Dropout model performs the prediction of the class for a new input differently from a classic model of DL. The predictive distribution, i.e., the estimated distribution of classes for a given input, is calculated by sampling the trained model with _T_ stochastic forward passes and average the results. 

![predictive.png]({{site.baseurl}}/_drafts/predictive.png)

We implement this estimation with the proposed **2-step predictive distribution** algorithm with GPU accelerated Monte-Carlo forward passes. The following figures consist of the input image with the true and predicted classes (top left), the predictive distribution and epistemic uncertainty (top right), and a stochastic forward passes distribution by class.


![High mutual information_0_18.png]({{site.baseurl}}/_drafts/High mutual information_0_18.png)


### Epistemic Uncertainty

The previous section has shown that the model can classify the images, but we are interested in measuring uncertainty. We start with **epistemic uncertainty**.  This kind o uncertainty represents the ignorance of the model parameters; it's also referred to as the **model uncertainty** because it can be interpreted as the **confidence in the prediction**. Epistemic uncertainty can reduce this type of uncertainty by adding new samples to our datasets.

We measure the epistemic uncertainty with the **Predictive Entropy** (H) or the **Mutual Inforation** (I):



### References

* [1] Gal, Y. (2016). Uncertainty in deep learning. University of Cambridge, 1(3), 4.
* [2] Gal, Y., & Ghahramani, Z. (2016, June). Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning (pp. 1050-1059). PMLR.
* [3] KENDALL, Alex; GAL, Yarin. What uncertainties do we need in bayesian deep learning for computer vision?. arXiv preprint arXiv:1703.04977, 2017.
* [4] DER KIUREGHIAN, Armen; DITLEVSEN, Ove. Aleatory or epistemic? Does it matter?. Structural safety, 2009, vol. 31, no 2, p. 105-112.


